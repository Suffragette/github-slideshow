{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8sfHwOeyIejyD5T71n8jr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suffragette/github-slideshow/blob/master/FINAL_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hMl1YSWtuchU"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n"
      ],
      "metadata": {
        "id": "PXAiciY3uv9g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2\n",
        "!pip install spark-nlp==3.3.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCew9aFOu2iw",
        "outputId": "6f309468-c275-4d8e-fab6-f31d8589ea86"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.1.2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.1.2) (0.10.9)\n",
            "Requirement already satisfied: spark-nlp==3.3.1 in /usr/local/lib/python3.10/dist-packages (3.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n"
      ],
      "metadata": {
        "id": "lh9jDfenvFHG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O train_data.csv https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_train.csv\n",
        "!wget -O test_data.csv https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_test.csv\n",
        "\n",
        "train_data = spark.read.csv(\"train_data.csv\", inferSchema=True, header=True)\n",
        "test_data = spark.read.csv(\"test_data.csv\", inferSchema=True, header=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7s6HVa8xSae",
        "outputId": "de89ad96-f98f-4fd9-c84d-b769ae13cd0e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-08 09:31:39--  https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14254503 (14M) [text/plain]\n",
            "Saving to: ‘train_data.csv’\n",
            "\n",
            "train_data.csv      100%[===================>]  13.59M  87.8MB/s    in 0.2s    \n",
            "\n",
            "2023-07-08 09:31:39 (87.8 MB/s) - ‘train_data.csv’ saved [14254503/14254503]\n",
            "\n",
            "--2023-07-08 09:31:39--  https://raw.githubusercontent.com/sebischair/Medical-Abstracts-TC-Corpus/main/medical_tc_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3586836 (3.4M) [text/plain]\n",
            "Saving to: ‘test_data.csv’\n",
            "\n",
            "test_data.csv       100%[===================>]   3.42M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-07-08 09:31:39 (32.6 MB/s) - ‘test_data.csv’ saved [3586836/3586836]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_d5isZBx-_M",
        "outputId": "b4926609-1dcb-49b4-b0e9-6ae3fc8be478"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['condition_label', 'medical_abstract']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import SentenceEmbeddings\n",
        "\n",
        "# Aggregate the BERT embeddings at the sentence level\n",
        "sentence_embeddings_bert = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"document\", \"bert_embeddings\"]) \\\n",
        "    .setOutputCol(\"bert_sentence_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "# Define the classifier\n",
        "classifier = ClassifierDLApproach() \\\n",
        "    .setInputCols([\"document\", \"bert_sentence_embeddings\"]) \\\n",
        "    .setOutputCol(\"class\") \\\n",
        "    .setLabelColumn(\"condition_label\") \\\n",
        "    .setMaxEpochs(10) \\\n",
        "    .setEnableOutputLogs(True)\n",
        "\n",
        "# Add the classifier to the pipeline\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    stopwords_cleaner,\n",
        "    word_embeddings,\n",
        "    bert_embeddings,\n",
        "    sentence_embeddings_bert,\n",
        "    classifier\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Transform the training and test data\n",
        "train_data = model.transform(train_data)\n",
        "test_data = model.transform(test_data)\n"
      ],
      "metadata": {
        "id": "djXPtskp4SRF"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}