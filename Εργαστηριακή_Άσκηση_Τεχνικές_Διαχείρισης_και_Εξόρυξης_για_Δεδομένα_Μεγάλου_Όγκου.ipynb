{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwfL/RhKBCbTpccTJ58NH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suffragette/github-slideshow/blob/master/%CE%95%CF%81%CE%B3%CE%B1%CF%83%CF%84%CE%B7%CF%81%CE%B9%CE%B1%CE%BA%CE%AE_%CE%86%CF%83%CE%BA%CE%B7%CF%83%CE%B7_%CE%A4%CE%B5%CF%87%CE%BD%CE%B9%CE%BA%CE%AD%CF%82_%CE%94%CE%B9%CE%B1%CF%87%CE%B5%CE%AF%CF%81%CE%B9%CF%83%CE%B7%CF%82_%CE%BA%CE%B1%CE%B9_%CE%95%CE%BE%CF%8C%CF%81%CF%85%CE%BE%CE%B7%CF%82_%CE%B3%CE%B9%CE%B1_%CE%94%CE%B5%CE%B4%CE%BF%CE%BC%CE%AD%CE%BD%CE%B1_%CE%9C%CE%B5%CE%B3%CE%AC%CE%BB%CE%BF%CF%85_%CE%8C%CE%B3%CE%BA%CE%BF%CF%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark==3.1.2\n",
        "!pip install spark-nlp==3.3.1\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import sparknlp\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark NLP\") \\\n",
        "    .master(\"local[4]\") \\\n",
        "    .config(\"spark.driver.memory\",\"16G\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n"
      ],
      "metadata": {
        "id": "Duo0jJKTReHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "OwpjxuaVTbkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load the data\n",
        "data = spark.read.csv(\"/content/drive/MyDrive/Chatgpt/Τεχνικές Διαχείρισης και Εξόρυξης για  Δεδομένα Μεγάλου Όγκου/Medical-Abstracts-TC-Corpus-main/medical_tc_train.csv\", inferSchema=True, header=True)\n",
        "data = spark.read.csv(\"/content/drive/MyDrive/Chatgpt/Τεχνικές Διαχείρισης και Εξόρυξης για  Δεδομένα Μεγάλου Όγκου/Medical-Abstracts-TC-Corpus-main/medical_tc_labels.csv\", inferSchema=True, header=True)\n",
        "data = spark.read.csv(\"/content/drive/MyDrive/Chatgpt/Τεχνικές Διαχείρισης και Εξόρυξης για  Δεδομένα Μεγάλου Όγκου/Medical-Abstracts-TC-Corpus-main/medical_tc_test.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "data.show()\n"
      ],
      "metadata": {
        "id": "jHn84pKyScca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n"
      ],
      "metadata": {
        "id": "Yy9BeFzAUbhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, StopWordsCleaner\n",
        "\n",
        "# Assemble the text column into a Document type column\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"medical_abstract\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Tokenize the document\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Normalize the tokens (remove punctuation and convert to lowercase)\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"clean_tokens\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "# Finish the preprocessing (convert the result back to a string column)\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"clean_tokens\"]) \\\n",
        "    .setOutputCols([\"clean_text\"]) \\\n",
        "    .setOutputAsArray(False) \\\n",
        "    .setCleanAnnotations(False)\n",
        "\n",
        "# Define the stages of the pipeline\n",
        "stages = [document_assembler, tokenizer, normalizer, stopwords_cleaner, finisher]\n",
        "\n",
        "# Create a pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline().setStages(stages)\n",
        "\n",
        "# Fit the pipeline to the data\n",
        "model = pipeline.fit(data)\n",
        "\n",
        "# Transform the data\n",
        "data_clean = model.transform(data)\n",
        "\n",
        "# Show the first few rows of the cleaned data\n",
        "data_clean.show()\n"
      ],
      "metadata": {
        "id": "-HotsT_5UH0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import WordEmbeddingsModel\n",
        "\n",
        "# Load the pre-trained word embeddings model\n",
        "embeddings = WordEmbeddingsModel.pretrained('glove_100d', 'en') \\\n",
        "    .setInputCols([\"document\", \"token\"]) \\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "# Add the embeddings to the stages of the pipeline\n",
        "stages = [document_assembler, tokenizer, normalizer, stopwords_cleaner, embeddings, finisher]\n",
        "\n",
        "# Create a pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline().setStages(stages)\n",
        "\n",
        "# Fit the pipeline to the data\n",
        "model = pipeline.fit(data)\n",
        "\n",
        "# Transform the data\n",
        "data_clean = model.transform(data)\n",
        "\n",
        "# Show the first few rows of the cleaned data\n",
        "data_clean.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjepGLJ2mnGA",
        "outputId": "ebfe81af-193b-4090-8c2a-d10a88c31f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|condition_label|    medical_abstract|            document|               token|          normalized|        clean_tokens|          embeddings|          clean_text|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|              3|Obstructive sleep...|[{document, 0, 15...|[{token, 0, 10, O...|[{token, 0, 10, O...|[{token, 0, 10, O...|[{word_embeddings...|Obstructive@sleep...|\n",
            "|              5|Neutrophil functi...|[{document, 0, 16...|[{token, 0, 9, Ne...|[{token, 0, 9, Ne...|[{token, 0, 9, Ne...|[{word_embeddings...|Neutrophil@functi...|\n",
            "|              5|A phase II study ...|[{document, 0, 18...|[{token, 0, 0, A,...|[{token, 0, 0, A,...|[{token, 2, 6, ph...|[{word_embeddings...|phase@II@study@co...|\n",
            "|              1|Flow cytometric D...|[{document, 0, 19...|[{token, 0, 3, Fl...|[{token, 0, 3, Fl...|[{token, 0, 3, Fl...|[{word_embeddings...|Flow@cytometric@D...|\n",
            "|              4|Paraneoplastic va...|[{document, 0, 68...|[{token, 0, 13, P...|[{token, 0, 13, P...|[{token, 0, 13, P...|[{word_embeddings...|Paraneoplastic@va...|\n",
            "|              1|Treatment of chil...|[{document, 0, 12...|[{token, 0, 8, Tr...|[{token, 0, 8, Tr...|[{token, 0, 8, Tr...|[{word_embeddings...|Treatment@childho...|\n",
            "|              1|Expression of maj...|[{document, 0, 80...|[{token, 0, 9, Ex...|[{token, 0, 9, Ex...|[{token, 0, 9, Ex...|[{word_embeddings...|Expression@major@...|\n",
            "|              1|Questionable role...|[{document, 0, 10...|[{token, 0, 11, Q...|[{token, 0, 11, Q...|[{token, 0, 11, Q...|[{word_embeddings...|Questionable@role...|\n",
            "|              5|Reversibility of ...|[{document, 0, 22...|[{token, 0, 12, R...|[{token, 0, 12, R...|[{token, 0, 12, R...|[{word_embeddings...|Reversibility@hep...|\n",
            "|              2|Current status of...|[{document, 0, 10...|[{token, 0, 6, Cu...|[{token, 0, 6, Cu...|[{token, 0, 6, Cu...|[{word_embeddings...|Current@status@du...|\n",
            "|              5|The importance of...|[{document, 0, 52...|[{token, 0, 2, Th...|[{token, 0, 2, Th...|[{token, 4, 13, i...|[{word_embeddings...|importance@congen...|\n",
            "|              1|Human papillomavi...|[{document, 0, 70...|[{token, 0, 4, Hu...|[{token, 0, 4, Hu...|[{token, 0, 4, Hu...|[{word_embeddings...|Human@papillomavi...|\n",
            "|              5|Gentamicin iontop...|[{document, 0, 98...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{word_embeddings...|Gentamicin@iontop...|\n",
            "|              1|Repeat hepatic re...|[{document, 0, 78...|[{token, 0, 5, Re...|[{token, 0, 5, Re...|[{token, 0, 5, Re...|[{word_embeddings...|Repeat@hepatic@re...|\n",
            "|              5|Evidence for intr...|[{document, 0, 15...|[{token, 0, 7, Ev...|[{token, 0, 7, Ev...|[{token, 0, 7, Ev...|[{word_embeddings...|Evidence@intralum...|\n",
            "|              5|Glutamic acid and...|[{document, 0, 13...|[{token, 0, 7, Gl...|[{token, 0, 7, Gl...|[{token, 0, 7, Gl...|[{word_embeddings...|Glutamic@acid@gam...|\n",
            "|              5|A useful techniqu...|[{document, 0, 14...|[{token, 0, 0, A,...|[{token, 0, 0, A,...|[{token, 2, 7, us...|[{word_embeddings...|useful@technique@...|\n",
            "|              5|The natural histo...|[{document, 0, 11...|[{token, 0, 2, Th...|[{token, 0, 2, Th...|[{token, 4, 10, n...|[{word_embeddings...|natural@history@u...|\n",
            "|              3|Hereditary intern...|[{document, 0, 17...|[{token, 0, 9, He...|[{token, 0, 9, He...|[{token, 0, 9, He...|[{word_embeddings...|Hereditary@intern...|\n",
            "|              5|Immune response o...|[{document, 0, 95...|[{token, 0, 5, Im...|[{token, 0, 5, Im...|[{token, 0, 5, Im...|[{word_embeddings...|Immune@response@p...|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import SentenceEmbeddings\n",
        "\n",
        "# Create a SentenceEmbeddings object\n",
        "sentence_embeddings = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "# Add the sentence_embeddings to the stages of the pipeline\n",
        "stages = [document_assembler, tokenizer, normalizer, stopwords_cleaner, embeddings, sentence_embeddings, finisher]\n",
        "\n",
        "# Create a pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline().setStages(stages)\n",
        "\n",
        "# Fit the pipeline to the data\n",
        "model = pipeline.fit(data)\n",
        "\n",
        "# Transform the data\n",
        "data_clean = model.transform(data)\n",
        "\n",
        "# Show the first few rows of the cleaned data\n",
        "data_clean.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NvSZogoqnVS",
        "outputId": "e7716a4a-649a-4479-f3f3-859b3047babf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|condition_label|    medical_abstract|            document|               token|          normalized|        clean_tokens|          embeddings| sentence_embeddings|          clean_text|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|              3|Obstructive sleep...|[{document, 0, 15...|[{token, 0, 10, O...|[{token, 0, 10, O...|[{token, 0, 10, O...|[{word_embeddings...|[{sentence_embedd...|Obstructive@sleep...|\n",
            "|              5|Neutrophil functi...|[{document, 0, 16...|[{token, 0, 9, Ne...|[{token, 0, 9, Ne...|[{token, 0, 9, Ne...|[{word_embeddings...|[{sentence_embedd...|Neutrophil@functi...|\n",
            "|              5|A phase II study ...|[{document, 0, 18...|[{token, 0, 0, A,...|[{token, 0, 0, A,...|[{token, 2, 6, ph...|[{word_embeddings...|[{sentence_embedd...|phase@II@study@co...|\n",
            "|              1|Flow cytometric D...|[{document, 0, 19...|[{token, 0, 3, Fl...|[{token, 0, 3, Fl...|[{token, 0, 3, Fl...|[{word_embeddings...|[{sentence_embedd...|Flow@cytometric@D...|\n",
            "|              4|Paraneoplastic va...|[{document, 0, 68...|[{token, 0, 13, P...|[{token, 0, 13, P...|[{token, 0, 13, P...|[{word_embeddings...|[{sentence_embedd...|Paraneoplastic@va...|\n",
            "|              1|Treatment of chil...|[{document, 0, 12...|[{token, 0, 8, Tr...|[{token, 0, 8, Tr...|[{token, 0, 8, Tr...|[{word_embeddings...|[{sentence_embedd...|Treatment@childho...|\n",
            "|              1|Expression of maj...|[{document, 0, 80...|[{token, 0, 9, Ex...|[{token, 0, 9, Ex...|[{token, 0, 9, Ex...|[{word_embeddings...|[{sentence_embedd...|Expression@major@...|\n",
            "|              1|Questionable role...|[{document, 0, 10...|[{token, 0, 11, Q...|[{token, 0, 11, Q...|[{token, 0, 11, Q...|[{word_embeddings...|[{sentence_embedd...|Questionable@role...|\n",
            "|              5|Reversibility of ...|[{document, 0, 22...|[{token, 0, 12, R...|[{token, 0, 12, R...|[{token, 0, 12, R...|[{word_embeddings...|[{sentence_embedd...|Reversibility@hep...|\n",
            "|              2|Current status of...|[{document, 0, 10...|[{token, 0, 6, Cu...|[{token, 0, 6, Cu...|[{token, 0, 6, Cu...|[{word_embeddings...|[{sentence_embedd...|Current@status@du...|\n",
            "|              5|The importance of...|[{document, 0, 52...|[{token, 0, 2, Th...|[{token, 0, 2, Th...|[{token, 4, 13, i...|[{word_embeddings...|[{sentence_embedd...|importance@congen...|\n",
            "|              1|Human papillomavi...|[{document, 0, 70...|[{token, 0, 4, Hu...|[{token, 0, 4, Hu...|[{token, 0, 4, Hu...|[{word_embeddings...|[{sentence_embedd...|Human@papillomavi...|\n",
            "|              5|Gentamicin iontop...|[{document, 0, 98...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{word_embeddings...|[{sentence_embedd...|Gentamicin@iontop...|\n",
            "|              1|Repeat hepatic re...|[{document, 0, 78...|[{token, 0, 5, Re...|[{token, 0, 5, Re...|[{token, 0, 5, Re...|[{word_embeddings...|[{sentence_embedd...|Repeat@hepatic@re...|\n",
            "|              5|Evidence for intr...|[{document, 0, 15...|[{token, 0, 7, Ev...|[{token, 0, 7, Ev...|[{token, 0, 7, Ev...|[{word_embeddings...|[{sentence_embedd...|Evidence@intralum...|\n",
            "|              5|Glutamic acid and...|[{document, 0, 13...|[{token, 0, 7, Gl...|[{token, 0, 7, Gl...|[{token, 0, 7, Gl...|[{word_embeddings...|[{sentence_embedd...|Glutamic@acid@gam...|\n",
            "|              5|A useful techniqu...|[{document, 0, 14...|[{token, 0, 0, A,...|[{token, 0, 0, A,...|[{token, 2, 7, us...|[{word_embeddings...|[{sentence_embedd...|useful@technique@...|\n",
            "|              5|The natural histo...|[{document, 0, 11...|[{token, 0, 2, Th...|[{token, 0, 2, Th...|[{token, 4, 10, n...|[{word_embeddings...|[{sentence_embedd...|natural@history@u...|\n",
            "|              3|Hereditary intern...|[{document, 0, 17...|[{token, 0, 9, He...|[{token, 0, 9, He...|[{token, 0, 9, He...|[{word_embeddings...|[{sentence_embedd...|Hereditary@intern...|\n",
            "|              5|Immune response o...|[{document, 0, 95...|[{token, 0, 5, Im...|[{token, 0, 5, Im...|[{token, 0, 5, Im...|[{word_embeddings...|[{sentence_embedd...|Immune@response@p...|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = data_clean.randomSplit([0.8, 0.2], seed=12345)\n"
      ],
      "metadata": {
        "id": "JAPSuaJTv5fy"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}